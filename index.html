<head>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    </head>

    <head>
        <style>
          body {
            margin-top: 50px; /* Keep the top margin at 0 */
            margin-right: 400px; /* Add a 20px margin to the right */
            margin-bottom: 20px; /* Add a 20px margin to the bottom */
            margin-left: 400px; /* Add a 20px margin to the left */
            background-color: white; /* Set the background color to white */
          }
        </style>
      </head>

<div style="text-align:center;">

    <h1>Reinforcement Learning With Dual-Observation for General Video Game Playing</h1>
    
    <p>
        <span style="font-size:20px;">Chengpeng Hu<sup>1</sup>, Ziqi Wang<sup>1</sup>,  Tianye Shu<sup>1</sup>, Hao Tong<sup>1,2</sup>,</span>
    <br>  
    <span style="font-size:20px;">Julian Togelius<sup>3</sup>,  Xin Yao<sup>1,2</sup> and  Jialin Liu<sup>1</sup></span>
    </p>
    
    
    
    <p>
    <sup>1 </sup><span style="font-size:20px;">Southern University of Science and Technology</span><br>
    <sup>2 </sup><span style="font-size:20px;">University of Birmingham</span><br>
    <sup>3 </sup><span style="font-size:20px;">New York University </span> 
    <br>
    <br>
    <span style="font-size:15px;">IEEE TRANSACTIONS ON GAMES, VOL. 15, NO. 2, JUNE 2023 </span> 
    </p>



</div>

<hr>
<h2>Abstract</h2>
<div  style="text-align:center;">


    <div style="text-align: justify; font-size:18px;">Reinforcement learning (RL) algorithms have performed well in playing challenging board and video games. More and more studies focus on improving the generalization ability of RL algorithms. The General Video Game AI (GVGAI) Learning Competition aims to develop agents capable of learning to play
        different game levels that were unseen during training. This article summarizes the five years GVGAI Learning Competition editions.
        At each edition, three new games were designed. The training and test levels were designed separately in the first three editions. Since 2020, three test levels of each game were generated by perturbing or combining two training levels. Then, we present a novel RL
        technique with dual-observation for general video game playing, assuming that it is more likely to observe similar local information in different levels rather than global information. Instead of directly
        inputting a single, raw pixel-based screenshot of the current game screen, our proposed general technique takes the encoded, transformed global, and local observations (LOs) of the game screen as
        two simultaneous inputs, aiming at learning local information for playing new levels. Our proposed technique is implemented with three state-of-the-art RL algorithms and tested on the game set
        of the 2020 GVGAI Learning Competition. Ablation studies show the outstanding performance of using encoded, transformed global, and LOs as input.</div>
    
<hr>
<div style="display: flex;">
<body>
    <div style="width: 70%;text-align: justify; font-size:18px;">
      <!-- Text content goes here -->
      <h3>Transformed Game Observations</h3>
      <p>Different areas of the game observation are of different importance. The avatar’s surrounding area usually has a more
        immediate and greater impact than the distant areas on the
        action selection. Therefore, Reinforcement Learning With Dual-Observation (DORL) is proposed to transform the
        received screenshot of the game screen into a GO and a local
        one. The transformation procedure is described as follows and
        illustrated on the right.
        <ol type="a">
        <li> A GO is not the direct use of the screenshot of the game
        screen. Instead, it is a \(h_g \times w_g\) RGB image in which the avatar is placed at centre, 
        where \(h_g = 2h-h_{tile}\) and \(w_g = 2w-w_{tile}\), \(w\) and \(h\) refers to the width and height of the original screenshot, \(w_{tile}\) and \(h_{tile}\) refer to the width and height of one single tile, respectively. 
        This guarantees that the original game screen is always contained in the global observation wherever the avatar moves. 
        <li> A LO is a \(h_l \times w_l\) RGB image (\(h_l\lt w\), \(h_l\lt h\)) in which the avatar is centred. In our experiments, \(h_l\) and \(w_l\) are set as \(50\), which means \(5 \times 5\) surrounding tiles are used.</p>
        </ol>
        </div>
<br>

    
  
    <div style="width: 60%;">
      <!-- Image goes here -->
      <img src="Fig4.png" alt="Example image", width="650px">
    </div>
  
  </div>

  <div style="width: 100%;text-align: justify; font-size:18px;">
    <!-- Text content goes here -->
      <h3>Tile-Vector Encoded Inputs</h3>
      <p>The GVGAI platform represents levels with tile-based
        maps. Those tiles, either for different types of sprites
        or for accessible areas, have a predefined, fixed size of 10 ×
        10 [17]. Hence, we convert the RGB images into tile-based
        matrices as input, on which one-hot encoding can be easily
        applied.
        All distinct tiles appeared in the training levels have been collected to build a tile dictionary, which is a set of \(\langle\)code, 
        reference vector \(\rangle\) tuples. The \(code \in \mathbf{N}\) identifies a unique \(10 \times 10\)-pixel tile image. 
        For each tile, 5 pixels are selected from its \(4^{th}\) row with a stride of \(2\), and another 5 are selected from its \(5^{th}\) column with a stride of \(2\). 
        For each selected \(10\) pixels, its averaged RGB value is calculated as \(\frac{1}{3}(v_R+v_G+v_B)\), where \(v_R\), \(v_G\) and \(v_B\) refer to the red, green and blue values of the pixel, respectively. 
        As a result, a reference vector composed of \(10\) averaged RGB values is obtained. 
        At every game tick during training or testing, the two RGB images of transformed global and local observations are considered as \(\frac{h_g}{10} \times \frac{w_g}{10}\) and \(\frac{h_l}{10} \times \frac{w_l}{10}\) grids of tiles, respectively. 
        The blank spaces added during transformation, which are not part of the game screen, are replaced by zeros. 
        For both grids, we can easily calculate the corresponding RGB-value vector for every tile inside and find its closest reference vector in the dictionary, measured by the Manhattan distance. 
        Assuming that some unexpected tiles may appear in the test levels, we find the closest tile with distance between reference vectors instead of simply finding the code with the same reference vector. 
        Then, the tile-based grids are converted to two matrices of tile codes, i.e., integers. Finally, one-hot encoding is applied. 
        Since there are no more than \(16\) tile types in each game, the size of final matrix is \(\frac{h_g}{10} \times \frac{w_g}{10} \times 16\) for the transformed global observation and \(\frac{h_l}{10} \times \frac{w_l}{10} \times 16\) for the local one.</p>

  </div>
</div>
  <h2>Reinforcement Learning With Dual-Observation demo</h2>
  below
</body>





<!-- <button onclick="toggleBib()" style="font-size: 20px; padding: 6px 10px;">Bibtex</button>
<div id="bibliography" style="display:none;">

    <p>Bibliography content goes here</p>
    
    </div>


<button onclick="toggleBib()" style="font-size: 20px; padding: 6px 10px;">Paper</button>


<script>
function toggleBib() {
  var x = document.getElementById("bibliography");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script> -->

